cracking the interview question: 

Horizontal cs Vertical Scaling

two ways to scale 

vertical scaling: 
	- increasing the resources of a specific node. for example, you might add additional memory to a server to improve its ability to handle load changes. 

Horizontal scaling: 
	- increasing the number of nodes. For example, you might add additional servers, thus decreasing the load on any one server. 

Load balancer
	- Typically, some front-end parts of scalable website will be thrown behind a load balancer. This allows a system to distribute the load evenly so that one server doesnt crash and take down the whole system. To do so, of course, you have to build out a network of cloned servers that all have essentially the same code and access the same data. 

Database denormalization and NoSQL 
- joins in a relational database such as SQL can get very slow as the system grows bigger. For this reason, you would generally avoid them. 

Denormalization is one part of this. Denormalization meaning adding redundant information into a database to speed up reads. For example, imagine a database describing projects and tasks (where a project can have multiple tasks). you might need to get the project name and the task information Rather than doing a join across these tables, you can store the project name within the task table (in addition to the project table)

Or you can go with NoSQL database. A NoSQL database does not support joins and might structure data in a different way. It is designed to scale better. 

Database Partitioning (Sharding)

Sharding means splitting the data across multiple machines while ensuring you have a way of figuring out which data is on which machine. 

A few common ways of partitioning include: 
	- Vertical Partitioning: This is basically partitioning by feature. For example, if you were building a social network, you might have one partition for table relating to profiles, another one for messages and so on. One drawback of this is that if one of these tables gets very large, you might need to repartition that database (possibly using a different partitioning scheme)

	- Key-based (or hash-based) partitioning: this uses some part of the data (for example an ID) to partition it. A very simple way to do this is to allocate N servers and put the data on mode (key, n). One issues with this is that the number of servers you have is effectively fixed. Adding additional servers means reallocating all the data - a very expensive task. 

	- Directory-based partitioning: in this scheme, you maintain a lookup table for where the data can be found. This makes it relatively reasy to add additional servers, but it comes with two major drawbacks. First the lookup table can be a single point of failure. Second, constantly accessing this table impacts performance. 


Caching 
------- 

An in-memory can deliver very rapid results. It is a simple key-value pairing and typically sits between your application layer and our data store. 

When an applicatiokn requests a piece of information, it first tries the cache. If the cache does not contain the key, it will then look up the data in the data store. (at this point, the data might -- or might not -- be sotre in the data store.)

When you cache, you might cache a query and its results directly. Or alternatively, you can cache the specific object (for example, a rendered version of a part of the website, or a list of the most recent blog posts)


Asynchronous Processing & Queues 
--------------------------------

Slow operations should ideally be done asynchronously. Otherwise, a user might get stuck waiting and waiting for a process to complete. 

In some cases, we can do this in advance (i.e. we can pre-process). For example, we might have a queue of jobs to be done that update some part of the website. If we were running a forum, one of these jobs might be to re-render a page that lists the most popular posts and the number of comments. That list might end up being slightly out of date, but thats perhaps okay. It's better than a user stuclk waiting on the website to load simply because someone added a new comment and invalidated the cache version of this page. 

In other cases, we might tell the user to wait and notify them when the process is done. 

Network Metrics 
	- bandwidth 
	- throughput 
	- latency 


MapReduce 

MapReduce program is typically used to process large amounts of data 

As its name suggests, a MapReduce program requires you to write a Map step and a Reduce step. The rest is handled by the system. 

Map: 
	Map takes in some data and emits a <key, value> pair 

Reduce: 
	Reduce takes a key and a set of associated values and "reduces" them in some way, emitting a new key and value. The results of this might be fed back into the Reduce program for more reducing. 

MapReduce allows us to do a lot of processing in parallel, which makes processing huge amounts of data more scalable. 


1. The system splits up the data across different machines. 
2. Each machine starts running the user-provided Map program. 
3. The Map program takes some data and emits a <key , value> pair. 
4. The system-provided Shuffle process reorganizes the data so that all <key, value> pairs associated with a given key go to the same machine, to be processed by Reduce. 
5. The user-provided Reduce program takes a key and a set of assoicated values and "reduces" them in some way, emitting a new key and value. The results of this might be fed back into the Reduce program for more reducing. 






