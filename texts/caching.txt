Caching 

- caching will enable you to make vastly better use of the resources you already have

- making otherwise unattainable product requirements feasible

locality of reference principle: recently requested data is likely to be requested again

short-term memory: it has a limited amount of space, but is typically faster than the original data source and contains the most recently accessed items

Goal: return data quickly without taxing downstream levels 

1. Application server cache 

Placing a cache directly on a request layer node enables the local storage of response data. Each time a request is made to the service, the node will quickly return local, cahced data if it exists. If it is not in the cache, the requesting node will query the data from disk. 
	- in memory cache ( very fast ) 
	- node's local disk (faster than going to network storage)

if the request layer is expanded to multiple nodes, its still quite possible to have each node host its own cache. However, if your load balancer randomly distributes requests across the nodes, the same request will go to different nodes, thus increasing cache misses. 

Two choices for overcoming this hurdle: 
	- global caches 
	- distributed caches

2. Distributed cache 

In a distributed cache, each of its nodes own part of the cached data. Typically, the cache is divided up using a consistent hashing function, such that if a request node is looking for a certain piece of data, it can quickly know where to look within the distributed cache to 

ADVANTAGE: of a distributed cache is the increased cache space that can be had just by adding nodes to the request pool.

DISADVANTAGE: remedying a missing node. 
Some distributed caches get around this by storing multiple copies of the data on different nodes; 

complication: you add or remove nodes from the request layer

3. Global Cache 
===============

A global cache: all the nodes use the same single cache space. This involves adding a server, or file store of some sort, faster than your original store and accessible by all the request layer nodes. 

Disadvantage: very easy to overwhelm a single cache as the number of clients and requests increase, but is very effective in some architectures 

two common forms of global caches depicted
	- when a cached response is not found in the cache, the cache itself becomes responsible for retrieving the missing piece of data from the underlying store

	- it is the responsibility of request nodes to retrieve any data that is not found in the cache.


two cases where the request nodes are responsible for retriving any data is better 
	- if the cache is being used for very large files, a low cache hit percentage would cause the cache buffer to become overwhelmed with cache misses; in this situation, it helps to have a large percentage of the total data set (or hot data set) in the cache. 

	- an architecture where the files stored in the cache are static and shouldn't be evicted. 

4. Content Distribution Network (CDN)

CDNs are a kind of cache that comes into play for sites serving large amounts of static media. IN a typical CDN setup, a request will first ask the CDN for a piece of static media; the CDN will serve that concent if it has it locally available. If it isnt available, the CDN will query the back-end servers for the file and then cache it locally and serve it to the requesting user. 

If the system we are building isnt yet large enough to have its own CDN, we can ease a future transition by serving the static media off a separate subdomain (e.g. static.yourservice.come) using a lightweight HTTP server like Nginx, and cutover the DNS from your servers to a CDN later. 

Cache Invalidation
==================

it require s some maintence for keeping cache coherent with the source of truth (e.g., database). If the data is modified in the database, it should be invalidated in the cache, if not, this can cause inconsistent application behaviour. 

Solving this problem is known as cache invalidation

Write-through cache:
-------------------- 
	- Under this scheme data is written into the cache and the corresponding database at the same time. 

	this scheme ensures that nothing will get lost in case of a crash 

	disadvantage: 
		- higher latency: every write operation must be done twice before returning success to the client


Write-around cache
------------------
data is writtent directly to permanent storage, bypassing the cache. 

This can reduce the cache being flooded with write operations that will not subsequentyly be re-read 

disadvantage: a read request for recently written data will create a "cache miss" and must be read from slower back-end storage and experience higher latency 

Write-back cache
----------------
Under this scheme, data is written to cache alone, and completion is immediately confirmed to the client. The write to the permanent storage is done after specified intervals or under certain conditions.

advantage: results in low latency and high throughput for write-intensive applications

disadvantage: this speed comes with the risk of data loss in case of a crash or other adverse event

Cache eviction policies
-----------------------

First In First Out (FIFO)

Last In First Out (LIFO)

Least Recently Used (LRU)

Most Recently Used (MRU)

Least Frequently Used (LFU)

Random Replacement (RR)



